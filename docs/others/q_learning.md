# $Q$-学習
強化学習の一種である $Q$-学習について簡単に述べておきます。

## 強化学習とは?
予め各アクションに対して報酬を設定しておき、AIが試行錯誤を繰り返しながら、報酬を最大化する行動を学習する機械学習の一種です。囲碁や将棋などのゲームAI、自動運転やロボット制御など様々な分野で用いられるAIに対して使用されています。

$Q$-学習は強化学習の中でももっとも古典的かつ基礎的なアプローチになります。

## 状態行動価値関数

まず、$Q$-学習の $Q$ とは何かという話ですが、これはマルコフ決定過程の理論において **状態行動価値関数(state-action-valued function)** と呼ばれるものになります。

[マルコフ決定過程](./mdp.md/#マルコフ決定過程の定義)において、初期値の状態だけでなく次の行動にも興味をもつと方策 $\pi$ に対して、状態行動価値関数

$$
Q^\pi(s,a)=\mathbb{E}_\pi\left[\sum_{n=0}^\infty\gamma^nR(X_n,Y_n,X_{n+1})\mid X_0=s,Y_0=a\right]
$$

を定義したくなります。なお細かい記号の定義は[理論的な話](./mdp.md)の方を見てください。

この関数には慣習的に $Q$ という文字を割り当てますが、$Q$-学習ではこの状態行動価値関数を最適化していくことにするので、ここから $Q$ という文字が来ています。

## $Q$-学習の概要

さて、もし[理論パート](./mdp.md)の方で示した最適方策 $\pi^*$ の存在を認めるのであれば、

$$
Q^{\pi^*}(s,a)=\max_\pi Q^\pi(s,a)
$$

が成立します。これは理論パートの内容を理解していればすぐに証明できます。そこで、与えられたマルコフ決定過程と割引率において、$Q(s,a)=Q^{\pi^*}(s,a)$ を求めるということが $Q$-学習の目標になります。$Q$-学習ではこれを動的計画法により実現します。つまり $Q$ 学習はおおむね次のように動作します。

1. 状態 $s\in S$ にいるときに今の状態行動価値関数の情報をもとに次の行動 $a\in A$ を決める。
2. $s,a$に従って確率的に次の状態に遷移し、報酬をもらう。
3. もらった報酬をフィードバックとして、状態行動価値関数を更新する。
4. 1.から3.までを繰り返す。

$Q$-学習は 1. で決める次の行動がある程度ランダム性をもっていれば最適解に収束することが証明されています。[^1]

[^1]:[Watkins, C.J.C.H., Dayan, P. Q-learning. Mach Learn 8, 279–292 (1992).](https://link.springer.com/article/10.1007/BF00992698)

## $Q$-学習のアルゴリズム (テーブルの更新)

ここまでくれば $Q$-学習のアルゴリズムは非常にシンプルです。以降マルコフ決定過程 $(S,A,T,R)$ と割引率 $0\leqq\gamma<1$ は固定しておきましょう。

まず最初は $Q(s,a)$ の値は分からないので適当に決めておきます。そして、状態 $s\in S$ において行動 $a\in A$ をとったときに状態 $s'\in S$ に遷移して、報酬 $r\in\mathbb{R}$ を受け取ったとしましょう。このとき、$Q(s,a)$ を以下の式で更新します。

$$
Q(s,a)\gets (1-\alpha) Q(s,a)+\alpha(r+\gamma\max_{a'\in A}Q(s',a'))
$$

ここで $0<\alpha<1$ は予め決めておいたパラメータで **学習率(learning rate)** と呼ばれます。気持ちとしては新しい情報に対する鋭敏さです。

これは今回デモで用いた AI の実装では `agent.py` の中の

```python
old_value = self.q_table[state, action]
next_max = np.max(self.q_table[next_state, :])

new_value = old_value + self.lr * (reward + self.gamma * next_max - old_value)
self.q_table[state, action] = new_value
```

という部分に相当します。確かに $Q$ に相当する `q_table` に更新式が代入されていることが見てとれるはずです。

## $Q$-学習のアルゴリズム (行動の決定)

アルゴリズムで残る問題は状態 $s\in S$ にいるときに次にどのような行動 $a\in A$ をどうやって決めるかです。これにはいくつかの方法があります。

まず今回デモで用いた AI では **$\varepsilon$-貪欲法** というアルゴリズムを用いました。これは予め $0\leqq\varepsilon\leqq 1$ を与えておき、

- 確率 $\varepsilon$ で次のアクションを一様ランダムに決める。
- 確率 $1-\varepsilon$ で今の $Q(s,a)$ を参照して最も値が高いアクションを選ぶ。

とするものです。$\varepsilon\geqq 0$ は最初のうちは $Q(s,a)$ の値は当てにならないので、大きくしておき、学習が進むにつれて下げていきました。`agent.py` での該当箇所は以下です。

```python
if random.uniform(0, 1) < self.epsilon:
    return random.randint(0, self.num_actions - 1)
else:
    return np.argmax(self.q_table[state, :])
```

この方法のメリットは AI の強さの調節が容易で、レース本番の AI では $\varepsilon$ の値を調整することで、でたらめな行動を誘発させて、すでに学習済みのAIの賢さを下げるという方向で難易度調整を実装しました。

今回は手軽さから $\varepsilon$-貪欲法を使用しましたが、もう少し手の込んだ方法として $Q(s,a)$ の値を加重平均するという方法もあります。これは例えば予めパラメータ $\beta>0$ を選んでおき、状態 $s\in S$ において行動 $a\in A$ をとる確率を

$$
\frac{\exp(\beta\cdot Q(s,a))}{\sum_{a'\in A}\exp(\beta\cdot Q(s,a'))}
$$

として決めるというものです。ここで、$\exp(x)=e^x$ です。

## $Q$-学習の限界

最後に $Q$-学習の限界について述べておきます。 $Q$-学習はマルコフ決定過程という理論モデルに裏打ちされているという点でうまくいくことがある程度理論保証された強力な手法です。

しかし一方で、そもそも実際に落とし込みたいモデルがマルコフ決定過程として定式化すべきなのかという点については何も触れていません。実際、今回のようなレースゲームではコースのどこら辺にいるかによって行動が変わるべきですが、マルコフ決定過程によるモデリングではそのような大域的な情報は拾えずに、ただ単に車の周りに壁がどのように見えているかしか考えていません。

また、マルコフ決定過程によるモデリングが正しいとしても、報酬や学習率などのハイパーパラメータを具体的にどう決めるかという問題があり、これに失敗すると $Q$-学習は成功しません。例えば、今回のレースゲームの例で言うと報酬設定を間違えてしまうとずっと止まったままの方が最適だと考えて一切動かないようなAIが完成してしまうことが往々にあります。

そのような問題から結局 $Q$-学習単体では AI の性能はあるところで頭打ちになってしまうことが非常に多いです。